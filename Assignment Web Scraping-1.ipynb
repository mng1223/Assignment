{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 428,
   "id": "7ca0397d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: bs4 in c:\\users\\hp\\anaconda3\\anaconda_new\\lib\\site-packages (0.0.1)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\hp\\anaconda3\\anaconda_new\\lib\\site-packages (from bs4) (4.12.2)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\hp\\anaconda3\\anaconda_new\\lib\\site-packages (from beautifulsoup4->bs4) (2.4)\n",
      "Requirement already satisfied: requests in c:\\users\\hp\\anaconda3\\anaconda_new\\lib\\site-packages (2.31.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\hp\\anaconda3\\anaconda_new\\lib\\site-packages (from requests) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\hp\\anaconda3\\anaconda_new\\lib\\site-packages (from requests) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\hp\\anaconda3\\anaconda_new\\lib\\site-packages (from requests) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\hp\\anaconda3\\anaconda_new\\lib\\site-packages (from requests) (2023.7.22)\n",
      "Requirement already satisfied: pandas in c:\\users\\hp\\anaconda3\\anaconda_new\\lib\\site-packages (2.0.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\hp\\anaconda3\\anaconda_new\\lib\\site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\hp\\anaconda3\\anaconda_new\\lib\\site-packages (from pandas) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\hp\\anaconda3\\anaconda_new\\lib\\site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: numpy>=1.21.0 in c:\\users\\hp\\anaconda3\\anaconda_new\\lib\\site-packages (from pandas) (1.24.3)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\hp\\anaconda3\\anaconda_new\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install bs4\n",
    "!pip install requests\n",
    "!pip install pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c47fc60",
   "metadata": {},
   "source": [
    "Importing required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 429,
   "id": "cfd50d36",
   "metadata": {},
   "outputs": [],
   "source": [
    "import bs4\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42364724",
   "metadata": {},
   "source": [
    "1) Write a python program to display all the header tags from wikipedia.org and make data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 430,
   "id": "6c1e60c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List all the header tags of wikipedia:\n",
      "\n",
      "<h1 class=\"firstHeading mw-first-heading\" id=\"firstHeading\" style=\"display: none\"><span class=\"mw-page-title-main\">Main Page</span></h1>\n",
      "\n",
      "<h1><span class=\"mw-headline\" id=\"Welcome_to_Wikipedia\">Welcome to <a href=\"/wiki/Wikipedia\" title=\"Wikipedia\">Wikipedia</a></span></h1>\n",
      "\n",
      "<h2 class=\"mp-h2\" id=\"mp-tfa-h2\"><span id=\"From_today.27s_featured_article\"></span><span class=\"mw-headline\" id=\"From_today's_featured_article\">From today's featured article</span></h2>\n",
      "\n",
      "<h2 class=\"mp-h2\" id=\"mp-dyk-h2\"><span class=\"mw-headline\" id=\"Did_you_know_...\">Did you know ...</span></h2>\n",
      "\n",
      "<h2 class=\"mp-h2\" id=\"mp-itn-h2\"><span class=\"mw-headline\" id=\"In_the_news\">In the news</span></h2>\n",
      "\n",
      "<h2 class=\"mp-h2\" id=\"mp-otd-h2\"><span class=\"mw-headline\" id=\"On_this_day\">On this day</span></h2>\n",
      "\n",
      "<h2 class=\"mp-h2\" id=\"mp-tfp-h2\"><span id=\"Today.27s_featured_picture\"></span><span class=\"mw-headline\" id=\"Today's_featured_picture\">Today's featured picture</span></h2>\n",
      "\n",
      "<h2 class=\"mp-h2\" id=\"mp-other\"><span class=\"mw-headline\" id=\"Other_areas_of_Wikipedia\">Other areas of Wikipedia</span></h2>\n",
      "\n",
      "<h2 class=\"mp-h2\" id=\"mp-sister\"><span id=\"Wikipedia.27s_sister_projects\"></span><span class=\"mw-headline\" id=\"Wikipedia's_sister_projects\">Wikipedia's sister projects</span></h2>\n",
      "\n",
      "<h2 class=\"mp-h2\" id=\"mp-lang\"><span class=\"mw-headline\" id=\"Wikipedia_languages\">Wikipedia languages</span></h2>\n"
     ]
    }
   ],
   "source": [
    "from urllib.request import urlopen\n",
    "\n",
    "html=urlopen(\"https://en.wikipedia.org/wiki/Main_Page\")\n",
    "\n",
    "bs=BeautifulSoup(html,\"html.parser\")\n",
    "\n",
    "titles=bs.find_all(['h1','h2','h3','h4','h5','h6'])\n",
    "\n",
    "print('List all the header tags of wikipedia:',*titles,sep='\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e221429",
   "metadata": {},
   "source": [
    "2) Write a python program to display list of respected former presidents of India(i.e. Name , Term of office) from https://presidentofindia.nic.in/former-presidents.htm and make data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 431,
   "id": "0f4a2ac5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            Name           Term of Office\n",
      "0           Shri Ram Nath Kovind  14th President of India\n",
      "1          Shri Pranab Mukherjee  13th President of India\n",
      "2   Smt Pratibha Devisingh Patil  12th President of India\n",
      "3         DR. A.P.J. Abdul Kalam  11th President of India\n",
      "4           Shri K. R. Narayanan  10th President of India\n",
      "5        Dr Shankar Dayal Sharma  9th  President of India\n",
      "6            Shri R Venkataraman   8th President of India\n",
      "7               Giani Zail Singh   7th President of India\n",
      "8      Shri Neelam Sanjiva Reddy   6th President of India\n",
      "9       Dr. Fakhruddin Ali Ahmed   5th President of India\n",
      "10  Shri Varahagiri Venkata Giri   4th President of India\n",
      "11              Dr. Zakir Husain   3rd President of India\n",
      "12  Dr. Sarvepalli Radhakrishnan   2nd President of India\n",
      "13           Dr. Rajendra Prasad   1st President of India\n"
     ]
    }
   ],
   "source": [
    "# URL of the website containing the list of former presidents\n",
    "url = \"https://presidentofindia.nic.in/former-presidents\"\n",
    "\n",
    "# Send a GET request to the website\n",
    "response = requests.get(url)\n",
    "\n",
    "# Parse the HTML content of the page\n",
    "soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "# Find the table containing the list of former presidents\n",
    "name_of_president = soup.find_all(\"div\",class_=\"president-listing\")\n",
    "time_period=soup.find_all('h5')\n",
    "\n",
    "# Extract data from the table and create lists for names and terms of office\n",
    "names = []\n",
    "for name in name_of_president:\n",
    "    name=name.h3.text\n",
    "    names.append(name)\n",
    "\n",
    "terms_office = []\n",
    "for tof in time_period:\n",
    "    tof=tof.text \n",
    "    terms_office.append(tof)\n",
    "\n",
    "# Create a DataFrame from the extracted data\n",
    "data = {\"Name\": names, \"Term of Office\": terms_office}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Print the DataFrame\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "331bfc4b",
   "metadata": {},
   "source": [
    "3) Write a python program to scrape cricket rankings from icc-cricket.com. You have to scrape and make data frame-\n",
    "a) Top 10 ODI teams in men’s cricket along with the records for matches, points and rating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 432,
   "id": "8aa13699",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               Team Matches Points Rating\n",
      "0        India\\nIND      49  5,839    119\n",
      "1    Australia\\nAUS      36  4,015    112\n",
      "2     Pakistan\\nPAK      32  3,525    110\n",
      "3  South Africa\\nSA      29  3,166    109\n",
      "4   New Zealand\\nNZ      38  4,007    105\n",
      "5      England\\nENG      34  3,377     99\n",
      "6     Sri Lanka\\nSL      43  3,943     92\n",
      "7   Bangladesh\\nBAN      40  3,574     89\n",
      "8  Afghanistan\\nAFG      26  2,170     83\n",
      "9   West Indies\\nWI      38  2,582     68\n"
     ]
    }
   ],
   "source": [
    "url = \"https://www.icc-cricket.com/rankings/mens/team-rankings/odi\"\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "team_data = []\n",
    "table = soup.find(\"table\", class_=\"table\")\n",
    "rows = table.find_all(\"tr\")\n",
    "\n",
    "for row in rows[1:11]:\n",
    "  cells = row.find_all(\"td\")\n",
    "  team = cells[1].text.strip()\n",
    "  matches = cells[2].text.strip()\n",
    "  points = cells[3].text.strip()\n",
    "  rating = cells[4].text.strip()\n",
    "  team_data.append([team, matches, points, rating])\n",
    "\n",
    "df = pd.DataFrame(team_data, columns=[\"Team\", \"Matches\", \"Points\", \"Rating\"])\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8e8fd6d",
   "metadata": {},
   "source": [
    "b) Top 10 ODI Batsmen along with the records of their team andrating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 433,
   "id": "78648efe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Batsman Team Rating\n",
      "0             Babar Azam  PAK    829\n",
      "1           Shubman Gill  IND    823\n",
      "2        Quinton de Kock   SA    769\n",
      "3       Heinrich Klaasen   SA    756\n",
      "4           David Warner  AUS    747\n",
      "5            Virat Kohli  IND    747\n",
      "6           Harry Tector  IRE    729\n",
      "7           Rohit Sharma  IND    725\n",
      "8  Rassie van der Dussen   SA    716\n",
      "9            Imam-ul-Haq  PAK    704\n"
     ]
    }
   ],
   "source": [
    "url = \"https://www.icc-cricket.com/rankings/mens/player-rankings/odi/batting\"\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "batsman_data = []\n",
    "table = soup.find(\"table\", class_=\"table\")\n",
    "rows = table.find_all(\"tr\")\n",
    "\n",
    "for row in rows[1:11]:\n",
    "  cells = row.find_all(\"td\")\n",
    "  batsman = cells[1].text.strip()\n",
    "  team = cells[2].text.strip()\n",
    "  rating = cells[3].text.strip()\n",
    "  batsman_data.append([batsman, team, rating])\n",
    "\n",
    "df = pd.DataFrame(batsman_data, columns=[\"Batsman\", \"Team\", \"Rating\"])\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e745f47d",
   "metadata": {},
   "source": [
    "c) Top 10 ODI bowlers along with the records of their team andrating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 434,
   "id": "06094c3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           Bowler Team Rating\n",
      "0  Josh Hazlewood  AUS    670\n",
      "1  Mohammed Siraj  IND    668\n",
      "2  Keshav Maharaj   SA    656\n",
      "3     Rashid Khan  AFG    654\n",
      "4     Trent Boult   NZ    653\n",
      "5   Mohammad Nabi  AFG    641\n",
      "6      Adam Zampa  AUS    635\n",
      "7      Matt Henry   NZ    634\n",
      "8   Kuldeep Yadav  IND    632\n",
      "9  Shaheen Afridi  PAK    625\n"
     ]
    }
   ],
   "source": [
    "url = \"https://www.icc-cricket.com/rankings/mens/player-rankings/odi/bowling\"\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "bowler_data = []\n",
    "table = soup.find(\"table\", class_=\"table\")\n",
    "rows = table.find_all(\"tr\")\n",
    "\n",
    "for row in rows[1:11]:\n",
    "  cells = row.find_all(\"td\")\n",
    "  bowler = cells[1].text.strip()\n",
    "  team = cells[2].text.strip()\n",
    "  rating = cells[3].text.strip()\n",
    "  bowler_data.append([bowler, team, rating])\n",
    "\n",
    "df = pd.DataFrame(bowler_data, columns=[\"Bowler\", \"Team\", \"Rating\"])\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11b4be95",
   "metadata": {},
   "source": [
    "4) Write a python program to scrape cricket rankings from icc-cricket.com. You have to scrape and make data frame-\n",
    "a) Top 10 ODI teams in women’s cricket along with the records for matches, points and rating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 435,
   "id": "ebc132cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               Team Matches Points Rating\n",
      "0    Australia\\nAUS      19  3,084    162\n",
      "1      England\\nENG      23  2,991    130\n",
      "2  South Africa\\nSA      21  2,446    116\n",
      "3        India\\nIND      18  1,745     97\n",
      "4   New Zealand\\nNZ      21  2,014     96\n",
      "5   West Indies\\nWI      18  1,610     89\n",
      "6     Sri Lanka\\nSL       9    714     79\n",
      "7   Bangladesh\\nBAN      11    816     74\n",
      "8     Thailand\\nTHA      11    753     68\n",
      "9     Pakistan\\nPAK      21  1,435     68\n"
     ]
    }
   ],
   "source": [
    "url = \"https://www.icc-cricket.com/rankings/womens/team-rankings/odi\"\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "team_data = []\n",
    "table = soup.find(\"table\", class_=\"table\")\n",
    "rows = table.find_all(\"tr\")\n",
    "\n",
    "for row in rows[1:11]:\n",
    "  cells = row.find_all(\"td\")\n",
    "  team = cells[1].text.strip()\n",
    "  matches = cells[2].text.strip()\n",
    "  points = cells[3].text.strip()\n",
    "  rating = cells[4].text.strip()\n",
    "  team_data.append([team, matches, points, rating])\n",
    "\n",
    "df = pd.DataFrame(team_data, columns=[\"Team\", \"Matches\", \"Points\", \"Rating\"])\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "546a76f4",
   "metadata": {},
   "source": [
    "b) Top 10 women’s ODI Batting players along with the records of their team and rating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 436,
   "id": "78f2c413",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                Batsman Team Rating\n",
      "0  Natalie Sciver-Brunt  ENG    807\n",
      "1           Beth Mooney  AUS    750\n",
      "2   Chamari Athapaththu   SL    736\n",
      "3       Laura Wolvaardt   SA    727\n",
      "4       Smriti Mandhana  IND    708\n",
      "5          Alyssa Healy  AUS    698\n",
      "6          Ellyse Perry  AUS    697\n",
      "7      Harmanpreet Kaur  IND    694\n",
      "8           Meg Lanning  AUS    662\n",
      "9        Marizanne Kapp   SA    642\n"
     ]
    }
   ],
   "source": [
    "url = \"https://www.icc-cricket.com/rankings/womens/player-rankings/odi/batting\"\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "batsman_data = []\n",
    "table = soup.find(\"table\", class_=\"table\")\n",
    "rows = table.find_all(\"tr\")\n",
    "\n",
    "for row in rows[1:11]:\n",
    "  cells = row.find_all(\"td\")\n",
    "  batsman = cells[1].text.strip()\n",
    "  team = cells[2].text.strip()\n",
    "  rating = cells[3].text.strip()\n",
    "  batsman_data.append([batsman, team, rating])\n",
    "\n",
    "df = pd.DataFrame(batsman_data, columns=[\"Batsman\", \"Team\", \"Rating\"])\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a84d1b77",
   "metadata": {},
   "source": [
    "c) Top 10 women’s ODI all-rounder along with the records of their team and rating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 437,
   "id": "f666279d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              Bowler Team Rating\n",
      "0  Sophie Ecclestone  ENG    746\n",
      "1     Shabnim Ismail   SA    680\n",
      "2      Jess Jonassen  AUS    662\n",
      "3       Megan Schutt  AUS    658\n",
      "4   Ashleigh Gardner  AUS    652\n",
      "5     Ayabonga Khaka   SA    640\n",
      "6         Kate Cross  ENG    628\n",
      "7    Hayley Matthews   WI    625\n",
      "8      Deepti Sharma  IND    607\n",
      "9     Marizanne Kapp   SA    600\n"
     ]
    }
   ],
   "source": [
    "url = \"https://www.icc-cricket.com/rankings/womens/player-rankings/odi/bowling\"\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "bowler_data = []\n",
    "table = soup.find(\"table\", class_=\"table\")\n",
    "rows = table.find_all(\"tr\")\n",
    "\n",
    "for row in rows[1:11]:\n",
    "  cells = row.find_all(\"td\")\n",
    "  bowler = cells[1].text.strip()\n",
    "  team = cells[2].text.strip()\n",
    "  rating = cells[3].text.strip()\n",
    "  bowler_data.append([bowler, team, rating])\n",
    "\n",
    "df = pd.DataFrame(bowler_data, columns=[\"Bowler\", \"Team\", \"Rating\"])\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75ca15bd",
   "metadata": {},
   "source": [
    "5) Write a python program to scrape mentioned news details from https://www.cnbc.com/world/?region=world and make data frame-\n",
    "i) Headline\n",
    "ii) Time\n",
    "iii) News Link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 438,
   "id": "9d2e7f55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                             Headline          Time  \\\n",
      "0   Pfizer swings to loss due to Paxlovid, Covid v...    13 Min Ago   \n",
      "1   Beer giant AB InBev beats forecasts, but Bud L...    15 Min Ago   \n",
      "2   Sam Bankman-Fried has one more shot at avoidin...    16 Min Ago   \n",
      "3   What the rise of homeowners associations means...    16 Min Ago   \n",
      "4   Euro zone inflation drops to two-year low in O...    1 Hour Ago   \n",
      "5   Zombie firms are filing for bankruptcy as the ...    1 Hour Ago   \n",
      "6   Winners and losers of the UAW talks with GM, F...    1 Hour Ago   \n",
      "7   Analyst calls: All the market-moving chatter f...   2 Hours Ago   \n",
      "8   Treasury yields fall as traders look to Fed me...   2 Hours Ago   \n",
      "9   The UK is gearing up for a pivotal summit on A...   2 Hours Ago   \n",
      "10  Taiwanese chip firm to build $5.3 billion semi...   2 Hours Ago   \n",
      "11  Hamas says it clashed with Israeli forces in n...   3 Hours Ago   \n",
      "12        CNBC Daily Open: Market bounce may not last   4 Hours Ago   \n",
      "13  Putin blames the West for Russian riot and 'de...   4 Hours Ago   \n",
      "14  BP shares down 4% after third-quarter profit p...   4 Hours Ago   \n",
      "15  A 'powder keg' in Europe is lurking in the sha...   4 Hours Ago   \n",
      "16  Oil could hit record high of $157 if Israel-Ha...   5 Hours Ago   \n",
      "17  Antisemitic mob rampage puts pressure on Mosco...   5 Hours Ago   \n",
      "18  European stocks higher as BP slips 4%; euro zo...   6 Hours Ago   \n",
      "19  Samsung expects memory chip demand to improve,...   7 Hours Ago   \n",
      "20  Bank of Japan increases flexibility on yield c...   7 Hours Ago   \n",
      "21  Australia wants to diversify its relationship ...  10 Hours Ago   \n",
      "22  Forget the megacaps. Bernstein likes these 'un...  10 Hours Ago   \n",
      "23  Is Meta a buy after the brutal tech sell-off? ...  10 Hours Ago   \n",
      "24  Apple announces new M3 chips and cuts the pric...  11 Hours Ago   \n",
      "25  New VF Corp. CEO talks plans to improve busine...  11 Hours Ago   \n",
      "26  As Singapore's aging population grows, busines...  11 Hours Ago   \n",
      "27           Cramer's Lightning Round: Merck is a buy  11 Hours Ago   \n",
      "28  CNBC Daily Open: Markets’ bounce may be short-...  11 Hours Ago   \n",
      "29  Japan stocks end higher after Bank of Japan al...  12 Hours Ago   \n",
      "\n",
      "                                            News Link  \n",
      "0   https://www.cnbc.com/2023/10/31/pfizer-pfe-q3-...  \n",
      "1   https://www.cnbc.com/2023/10/31/beer-giant-ab-...  \n",
      "2   https://www.cnbc.com/2023/10/31/sam-bankman-fr...  \n",
      "3   https://www.cnbc.com/2023/10/31/what-the-rise-...  \n",
      "4   https://www.cnbc.com/2023/10/31/euro-zone-infl...  \n",
      "5   https://www.cnbc.com/2023/10/31/zombie-firm-ba...  \n",
      "6   https://www.cnbc.com/2023/10/31/uaw-talks-with...  \n",
      "7   https://www.cnbc.com/2023/10/31/analyst-calls-...  \n",
      "8   https://www.cnbc.com/2023/10/31/us-treasury-yi...  \n",
      "9   https://www.cnbc.com/2023/10/31/what-you-need-...  \n",
      "10  https://www.cnbc.com/2023/10/31/psmc-sbi-holdi...  \n",
      "11  https://www.cnbc.com/2023/10/31/israel-hamas-w...  \n",
      "12  https://www.cnbc.com/2023/10/31/stock-markets-...  \n",
      "13  https://www.cnbc.com/2023/10/31/ukraine-war-li...  \n",
      "14  https://www.cnbc.com/2023/10/31/oil-major-bp-m...  \n",
      "15  https://www.cnbc.com/2023/10/31/israel-hamas-w...  \n",
      "16  https://www.cnbc.com/2023/10/31/oil-could-hit-...  \n",
      "17  https://www.cnbc.com/2023/10/31/anti-israel-ai...  \n",
      "18  https://www.cnbc.com/2023/10/31/european-marke...  \n",
      "19  https://www.cnbc.com/2023/10/31/samsung-q3-ear...  \n",
      "20  https://www.cnbc.com/2023/10/31/boj-increases-...  \n",
      "21  https://www.cnbc.com/2023/10/31/australia-want...  \n",
      "22  https://www.cnbc.com/2023/10/31/semiconductors...  \n",
      "23  https://www.cnbc.com/2023/10/31/is-meta-a-buy-...  \n",
      "24  https://www.cnbc.com/2023/10/30/apple-announce...  \n",
      "25  https://www.cnbc.com/2023/10/30/new-vf-corp-ce...  \n",
      "26  https://www.cnbc.com/2023/10/30/as-singapores-...  \n",
      "27  https://www.cnbc.com/2023/10/30/cramers-lightn...  \n",
      "28  https://www.cnbc.com/2023/10/31/stock-markets-...  \n",
      "29  https://www.cnbc.com/2023/10/31/asia-markets-p...  \n"
     ]
    }
   ],
   "source": [
    "# Send a GET request to the website\n",
    "url = \"https://www.cnbc.com/world/?region=world\"\n",
    "response = requests.get(url)\n",
    "\n",
    "# Create a BeautifulSoup object to parse the HTML content\n",
    "soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "# Find all the news articles on the page\n",
    "articles = soup.find_all(\"a\", class_=\"LatestNews-headline\")\n",
    "time_stamp = soup.find_all(\"time\", class_=\"LatestNews-timestamp\")\n",
    "\n",
    "# Initialize empty lists to store the scraped data\n",
    "headlines = []\n",
    "links = []\n",
    "times = []\n",
    "\n",
    "# Loop through each article and extract the required information\n",
    "for article in articles:\n",
    "  # Extract the headline\n",
    "  headline = article.text.strip()\n",
    "  headlines.append(headline)\n",
    "\n",
    "  # Extract the news link\n",
    "  link = article.get(\"href\")\n",
    "  links.append(link)\n",
    "\n",
    "for time in time_stamp:\n",
    " # Extract the time\n",
    "  time= time.text \n",
    "  times.append(time)\n",
    "\n",
    "# Create a dataframe using the scraped data\n",
    "df = pd.DataFrame( {\"Headline\": headlines,\"Time\": times,\"News Link\": links})\n",
    "\n",
    "# Print the dataframe\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85250fa7",
   "metadata": {},
   "source": [
    "6) Write a python program to scrape the details of most downloaded articles from AI in last 90 days.https://www.journals.elsevier.com/artificial-intelligence/most-downloaded-articles Scrape below mentioned details and make data frame-\n",
    "i) Paper Title\n",
    "ii) Authors\n",
    "iii) Published Date\n",
    "iv) Paper URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 439,
   "id": "c392b04d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                          Paper Title  \\\n",
      "0                                    Reward is enough   \n",
      "1   Explanation in artificial intelligence: Insigh...   \n",
      "2              Creativity and artificial intelligence   \n",
      "3   Conflict-based search for optimal multi-agent ...   \n",
      "4   Knowledge graphs as tools for explainable mach...   \n",
      "5   Law and logic: A review from an argumentation ...   \n",
      "6   Between MDPs and semi-MDPs: A framework for te...   \n",
      "7   Explaining individual predictions when feature...   \n",
      "8       Multiple object tracking: A literature review   \n",
      "9   A survey of inverse reinforcement learning: Ch...   \n",
      "10  Evaluating XAI: A comparison of rule-based and...   \n",
      "11  Explainable AI tools for legal reasoning about...   \n",
      "12            Hard choices in artificial intelligence   \n",
      "13  Assessing the communication gap between AI mod...   \n",
      "14  Explaining black-box classifiers using post-ho...   \n",
      "15  The Hanabi challenge: A new frontier for AI re...   \n",
      "16              Wrappers for feature subset selection   \n",
      "17  Artificial cognition for social human–robot in...   \n",
      "18  A review of possible effects of cognitive bias...   \n",
      "19  The multifaceted impact of Ada Lovelace in the...   \n",
      "20  Robot ethics: Mapping the issues for a mechani...   \n",
      "21          Reward (Mis)design for autonomous driving   \n",
      "22  Planning and acting in partially observable st...   \n",
      "23  What do we want from Explainable Artificial In...   \n",
      "\n",
      "                                              Authors  Published Date  \\\n",
      "0   David Silver, Satinder Singh, Doina Precup, Ri...    October 2021   \n",
      "1                                          Tim Miller   February 2019   \n",
      "2                                   Margaret A. Boden     August 1998   \n",
      "3   Guni Sharon, Roni Stern, Ariel Felner, Nathan ...   February 2015   \n",
      "4                      Ilaria Tiddi, Stefan Schlobach    January 2022   \n",
      "5                      Henry Prakken, Giovanni Sartor    October 2015   \n",
      "6     Richard S. Sutton, Doina Precup, Satinder Singh     August 1999   \n",
      "7           Kjersti Aas, Martin Jullum, Anders Løland  September 2021   \n",
      "8                Wenhan Luo, Junliang Xing and 4 more      April 2021   \n",
      "9                       Saurabh Arora, Prashant Doshi     August 2021   \n",
      "10  Jasper van der Waa, Elisabeth Nieuwburg, Anita...   February 2021   \n",
      "11  Joe Collenette, Katie Atkinson, Trevor Bench-C...      April 2023   \n",
      "12   Roel Dobbe, Thomas Krendl Gilbert, Yonatan Mintz   November 2021   \n",
      "13  Oskar Wysocki, Jessica Katharine Davies and 5 ...      March 2023   \n",
      "14  Eoin M. Kenny, Courtney Ford, Molly Quinn, Mar...        May 2021   \n",
      "15          Nolan Bard, Jakob N. Foerster and 13 more      March 2020   \n",
      "16                         Ron Kohavi, George H. John   December 1997   \n",
      "17      Séverin Lemaignan, Mathieu Warnier and 3 more       June 2017   \n",
      "18    Tomáš Kliegr, Štěpán Bahník, Johannes Fürnkranz       June 2021   \n",
      "19                             Luigia Carlucci Aiello       June 2016   \n",
      "20             Patrick Lin, Keith Abney, George Bekey      April 2011   \n",
      "21     W. Bradley Knox, Alessandro Allievi and 3 more      March 2023   \n",
      "22  Leslie Pack Kaelbling, Michael L. Littman, Ant...        May 1998   \n",
      "23             Markus Langer, Daniel Oster and 6 more       July 2021   \n",
      "\n",
      "                                            Paper URL  \n",
      "0   https://www.sciencedirect.com/science/article/...  \n",
      "1   https://www.sciencedirect.com/science/article/...  \n",
      "2   https://www.sciencedirect.com/science/article/...  \n",
      "3   https://www.sciencedirect.com/science/article/...  \n",
      "4   https://www.sciencedirect.com/science/article/...  \n",
      "5   https://www.sciencedirect.com/science/article/...  \n",
      "6   https://www.sciencedirect.com/science/article/...  \n",
      "7   https://www.sciencedirect.com/science/article/...  \n",
      "8   https://www.sciencedirect.com/science/article/...  \n",
      "9   https://www.sciencedirect.com/science/article/...  \n",
      "10  https://www.sciencedirect.com/science/article/...  \n",
      "11  https://www.sciencedirect.com/science/article/...  \n",
      "12  https://www.sciencedirect.com/science/article/...  \n",
      "13  https://www.sciencedirect.com/science/article/...  \n",
      "14  https://www.sciencedirect.com/science/article/...  \n",
      "15  https://www.sciencedirect.com/science/article/...  \n",
      "16  https://www.sciencedirect.com/science/article/...  \n",
      "17  https://www.sciencedirect.com/science/article/...  \n",
      "18  https://www.sciencedirect.com/science/article/...  \n",
      "19  https://www.sciencedirect.com/science/article/...  \n",
      "20  https://www.sciencedirect.com/science/article/...  \n",
      "21  https://www.sciencedirect.com/science/article/...  \n",
      "22  https://www.sciencedirect.com/science/article/...  \n",
      "23  https://www.sciencedirect.com/science/article/...  \n"
     ]
    }
   ],
   "source": [
    "# Send a GET request to the URL\n",
    "url = \"https://www.journals.elsevier.com/artificial-intelligence/most-downloaded-articles\"\n",
    "response = requests.get(url)\n",
    "\n",
    "# Create a BeautifulSoup object to parse the HTML content\n",
    "soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "# Find the article details\n",
    "articles_titles = soup.find_all(\"h2\", class_=\"sc-1qrq3sd-1 gRGSUS sc-1nmom32-0 sc-1nmom32-1 btcbYu goSKRg\")\n",
    "articles_authors = soup.find_all(\"span\", class_=\"sc-1w3fpd7-0 dnCnAO\")\n",
    "articles_dates = soup.find_all(\"span\", class_=\"sc-1thf9ly-2 dvggWt\")\n",
    "articles_urls = soup.find_all(\"a\", class_=\"sc-5smygv-0 fIXTHm\")\n",
    "\n",
    "\n",
    "# Initialize empty lists to store the scraped data\n",
    "titles = []\n",
    "authors = []\n",
    "dates = []\n",
    "urls = []\n",
    "\n",
    "# Iterate over each article in the article details:\n",
    "\n",
    "for title in articles_titles:\n",
    "  # Scrape the title\n",
    "  title = title.text.strip()\n",
    "  titles.append(title)\n",
    "\n",
    "for author in articles_authors:  \n",
    "  # Scrape the authors\n",
    "  author = author.text.strip()\n",
    "  authors.append(author)\n",
    "\n",
    "for date in articles_dates:\n",
    "  # Scrape the published date\n",
    "  date = date.text.strip()\n",
    "  dates.append(date)\n",
    "\n",
    "for url in articles_urls:\n",
    "  # Scrape the paper URL\n",
    "  url = url.get(\"href\")\n",
    "  urls.append(url)\n",
    "\n",
    "# Create a dataframe with the scraped data\n",
    "data = {\n",
    "  \"Paper Title\": titles,\n",
    "  \"Authors\": authors,\n",
    "  \"Published Date\": dates,\n",
    "  \"Paper URL\": urls\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Print the dataframe\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbc60532",
   "metadata": {},
   "source": [
    "7) Write a python program to scrape mentioned details from dineout.co.in and make data frame-\n",
    "i) Restaurant name\n",
    "ii) Cuisine\n",
    "iii) Location\n",
    "iv) Ratings\n",
    "v) Image URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 440,
   "id": "5faa9316",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    Restaurant Name  \\\n",
      "0                   Castle Barbeque   \n",
      "1                        Cafe Knosh   \n",
      "2                       India Grill   \n",
      "3              The Barbeque Company   \n",
      "4                    Delhi Barbeque   \n",
      "5  The Monarch - Bar Be Que Village   \n",
      "6                 Indian Grill Room   \n",
      "7                The Barbeque Times   \n",
      "\n",
      "                                             Cuisine  \\\n",
      "0                              Chinese, North Indian   \n",
      "1                               Italian, Continental   \n",
      "2                              North Indian, Italian   \n",
      "3                              North Indian, Chinese   \n",
      "4                                       North Indian   \n",
      "5                                       North Indian   \n",
      "6                              North Indian, Mughlai   \n",
      "7   North Indian, Continental, Chinese, South Indian   \n",
      "\n",
      "                                            Location Ratings  \\\n",
      "0                     Connaught Place, Central Delhi       4   \n",
      "1  The Leela Ambience Convention Hotel,Shahdara, ...     4.3   \n",
      "2               Hilton Garden Inn,Saket, South Delhi     3.9   \n",
      "3                 Gardens Galleria,Sector 38A, Noida     3.9   \n",
      "4     Taurus Sarovar Portico,Mahipalpur, South Delhi     3.7   \n",
      "5  Indirapuram Habitat Centre,Indirapuram, Ghaziabad     3.8   \n",
      "6   Suncity Business Tower,Golf Course Road, Gurgaon     4.3   \n",
      "7              M2K Corporate Park,Sector 51, Gurgaon     4.1   \n",
      "\n",
      "                                           Image URL  \n",
      "0  https://im1.dineout.co.in/images/uploads/resta...  \n",
      "1  https://im1.dineout.co.in/images/uploads/resta...  \n",
      "2  https://im1.dineout.co.in/images/uploads/resta...  \n",
      "3  https://im1.dineout.co.in/images/uploads/resta...  \n",
      "4  https://im1.dineout.co.in/images/uploads/resta...  \n",
      "5  https://im1.dineout.co.in/images/uploads/resta...  \n",
      "6  https://im1.dineout.co.in/images/uploads/resta...  \n",
      "7  https://im1.dineout.co.in/images/uploads/resta...  \n"
     ]
    }
   ],
   "source": [
    "# Send a GET request to the website\n",
    "url = \"https://www.dineout.co.in/delhi-restaurants/buffet-special\"\n",
    "response = requests.get(url)\n",
    "\n",
    "# Create a BeautifulSoup object to parse the HTML content\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "# Find the elements containing the details you want to scrape\n",
    "restaurant_names = soup.find_all('a',class_='restnt-name ellipsis')\n",
    "cuisines= soup.find_all('span', class_='double-line-ellipsis')\n",
    "ratings = soup.find_all('div', class_='restnt-rating rating-4')\n",
    "locations = soup.find_all('div', class_='restnt-loc ellipsis')\n",
    "image_urls = soup.find_all('img', class_='no-img')\n",
    "\n",
    "# Create empty lists to store the scraped data\n",
    "restaurant_list = []\n",
    "cuisine_list = []\n",
    "location_list = []\n",
    "rating_list = []\n",
    "image_url_list = []\n",
    "\n",
    "# Extract the data from the elements and append them to the respective lists\n",
    "for name in restaurant_names:\n",
    "  restaurant_list.append(name.text.strip())\n",
    "\n",
    "for cuisine in cuisines:\n",
    "  cuisine_list.append(cuisine.text.split('|')[1])\n",
    "\n",
    "for location in locations:\n",
    "  location_list.append(location.text.strip())\n",
    "\n",
    "for rating in ratings:\n",
    "  rating_list.append(rating.text.strip())\n",
    "\n",
    "for image in image_urls:\n",
    "  image_url_list.append(image.get('data-src'))\n",
    "\n",
    "# Create a dictionary from the lists\n",
    "data = {\n",
    "  'Restaurant Name': restaurant_list,\n",
    "  'Cuisine': cuisine_list,\n",
    "  'Location': location_list,\n",
    "  'Ratings': rating_list,\n",
    "  'Image URL': image_url_list\n",
    "}\n",
    "\n",
    "# Create a dataframe from the dictionary\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Print the dataframe\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1445493b",
   "metadata": {},
   "outputs": [],
   "source": [
    "Thankyou"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
